# -*- coding: utf-8 -*-
"""bert_embeddings_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hFiiFL2m1pPC6Z12447956o5NMGB_T28
"""

import torch
import pandas as pd
from transformers import BertTokenizer, BertModel

# Load pre-trained BERT model and tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

# Load your dataset
file_path = '.csv'
df = pd.read_csv(file_path)

# Assuming your text column is named 'text_column'
text_column_name = 'overview'

# Tokenize the text column
tokenized_texts = df[text_column_name].apply(lambda x: tokenizer.encode(x, add_special_tokens=True))

# Pad tokenized sequences to the same length
max_len = max(len(tokens) for tokens in tokenized_texts)
padded_sequences = [tokens + [0] * (max_len - len(tokens)) for tokens in tokenized_texts]

# Convert to PyTorch tensors
input_ids = torch.tensor(padded_sequences)

# Use BERT model to get embeddings
with torch.no_grad():
    outputs = model(input_ids)

# Extract BERT embeddings (CLS token) for each row
bert_embeddings = outputs.last_hidden_state[:, 0, :].numpy()

# Create a new DataFrame column with BERT embeddings
embedding_column_name = 'bert_embeddings'
df[embedding_column_name] = list(bert_embeddings)

df

df.to_csv('output_file.csv',index=False)

from sklearn.metrics.pairwise import cosine_similarity

# Function to perform similarity search based on movie name
def similarity_search(query_movie_name, df, metric='cosine_similarity'):
    # Get the overview for the query movie
    query_overview = df[df['title'] == query_movie_name]['overview'].values[0]

    # Tokenize the query overview
    query_tokens = tokenizer.encode(query_overview, add_special_tokens=True)

    # Pad tokenized query to the same length
    query_padded = query_tokens + [0] * (max_len - len(query_tokens))

    # Convert to PyTorch tensor
    query_input_ids = torch.tensor([query_padded])

    # Use BERT model to get the embedding for the query
    with torch.no_grad():
        query_embedding = model(query_input_ids).last_hidden_state[:, 0, :].numpy()

    # Flatten the query_embedding
    query_embedding_flat = query_embedding.flatten()

    # Calculate similarity with other movies
    if metric == 'cosine_similarity':
        # Ensure that the dimensions are consistent for cosine_similarity
        query_embedding_flat = query_embedding_flat.reshape(1, -1)
        movie_embeddings_flat = np.vstack(df[embedding_column_name].apply(lambda x: x.flatten()))
        similarities = cosine_similarity(query_embedding_flat, movie_embeddings_flat)
    else:
        # Use Euclidean distance as an alternative
        similarities = -((query_embedding_flat - df[embedding_column_name].apply(lambda x: x.flatten())) ** 2).sum(axis=1)

    # Add similarity scores to the DataFrame
    df['similarity'] = similarities.flatten()

    # Sort by similarity
    similar_movies = df.sort_values(by='similarity', ascending=False)

    return similar_movies[['title', 'overview', 'similarity']]

# Example usage
query_movie = 'Pirates of the Caribbean: On Stranger Tides'
similar_movies = similarity_search(query_movie, df)

# Display the results
print(f"Movies similar to '{query_movie}':")
print(similar_movies[['title', 'similarity']])

